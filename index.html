<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Contamination Detection for VLMs using Multi‑Modal Semantic Perturbations">
  <meta name="keywords" content="contamination detection, VLM, multi-modal semantic perturbation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>mm-semantic-perturbation</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="stylesheet" href="./static/css/video-player.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.8.0/gradio.js"></script>

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }

.author-block a {
    color: #008AD7;
    font-weight: normal;
}

/* Adjust the vertical alignment and font size of the superscript */
.author-block a sup {
    vertical-align: baseline;
    position: relative;
    top: -0.3em; /* Adjusts the position slightly above the baseline */
    right: -0.1em; /* Adjusts the position slightly to the right */
    font-size: smaller; /* Makes the font size smaller if needed */
}

/* Color code superscript numbers */
.author-block a sup.sup-1,
.author-block sup.sup-1 {
    color: #e74c3c; /* Red for 1 */
}

.author-block a sup.sup-2,
.author-block sup.sup-2 {
    color: #3498db; /* Blue for 2 */
}

.author-block a sup.sup-3,
.author-block sup.sup-3 {
    color: #2ecc71; /* Green for 3 */
}

/* Spacing for affiliation superscripts */
.author-block sup {
    margin-right: 0.2em;
}
  
    .no-sort {
        cursor: default;
        pointer-events: none;
        background-image: none !important; /* Remove the sort arrow */
    }
  


</style>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h3 class="title is-3 publication-title">Contamination Detection for VLMs using Multi‑Modal Semantic Perturbations
            </h3>
            <h5 class="subtitle is-4 publication-awards">arXiv 2025</h5>
            <div class="is-size-4 publication-authors">

              <span class="author-block">
                <a href="https://jadenpark0.github.io/">Jaden Park<sup class="sup-1">1</sup></a>,
              </span>


              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~mucai/">Mu Cai<sup class="sup-1">1</sup></a>,
              </span>

              <span class="author-block">
                <a href="https://yaof20.github.io/">Feng Yao<sup class="sup-2">2</sup></a>,
              </span>

              <span class="author-block">
                <a href="https://shangjingbo1226.github.io/">Jingbo Shang<sup class="sup-2">2</sup></a>,
              </span>

              <span class="author-block">
                <a href="https://sites.google.com/view/soochahnlee/home">Soochahn Lee<sup class="sup-3">3</sup></a>,
              </span>
              
              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#008AD7;font-weight:normal;">Yong Jae Lee<sup class="sup-1">1</sup></a>
              </span>
            </div>


            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup class="sup-1">1</sup> University of Wisconsin-Madison</span>
              <span class="author-block"><sup class="sup-2">2</sup> University of California, San Diego</span>
              <span class="author-block"><sup class="sup-3">3</sup> Kookmin University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.02763" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://github.com/Vinoground/Vinoground" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/HanSolo9682/Vinoground" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#leaderboard"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-chart-bar"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-left">
          We introduce <b>Multi-modal Semantic Perturbation</b>, a pipeline to create perturbed benchmarks that can be used to detect data contamination in VLMs.
        </h4>
        <div style="text-align: center; margin-top: 2rem;">
          <img id="teaser" width="85%" src="images/main_fig.png">
          <p style="margin-top: 1rem;">Example of our multi-modal semantic perturbation pipeline applied to RealWorldQA benchmark. Using ControlNet trained with Flux models, a new speed limit sign is generated, changing the correct answer from (B) to (C) while preserving the original image's overall composition. A contaminated model that has memorized the original question is likely to fail on the perturbed version.</p>
        </div>
        <br>
        <h4 class="subtitle has-text-left">
          This perturbation pipeline generates image-question pairs with the original image composition intact but modified slightly so that the answer is changed.
          <br><br>
          The perturbed benchmark will have a similar or lower difficulty than the original benchmark, meaning clean models that truly generalize should perform better. However, we discover that contaminated models consistently underperform, showing dramatic performance drops up to -45%.
        </h4>
      </div>
    </div>
  </section>

  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in Vision–Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to <strong><em>test-set leakage</em></strong>. While prior work has proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for <strong><em>contaminated VLMs</em></strong> remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on <strong><em>multi-modal semantic perturbation</em></strong>, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple contamination strategies, confirming its robustness and effectiveness.  
            </p>
  
          </div>
        </div>
      </div>
        
    </div>
  </section>



  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Pipeline of Multi-modal Semantic Perturbation </h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->    
  <div class="container is-max-desktop">
  
    <div class="columns is-centered">
      <div class="column is-full-width">

        <centering>
          <div style="text-align: center;">
            <img id="teaser" src="images/pipeline.png">  
          </div>
        </centering> 
        
        <div class="content has-text-justified"> 
          <ol>
            <li>First, we randomly sample a different answer choice from the original question, and feed the original image along with the question and the new answer to an LLM.</li>
            <li>This generates a dense caption that can be used to prompt ControlNet to generate a perturbed image with the new answer, while preserving the image's overall composition.</li>
            <li>We manually verify the perturbed image-question pairs to ensure they are valid and the answer is changed. This process can be automated using a strong reasoning model like <em>o3</em>.</li>
          </ol>
          <br>
          We verify that contaminated models consistently underperform on the perturbed benchmark with varying epochs, training strategies, and model architectures.
        </div> 
      </div>
  </section>

  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Contamination Detection Results </h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->    
  <div class="container is-max-desktop">
  
    We use  <a href="https://huggingface.co/datasets/nirajandhakal/realworldqa">RealWorldQA</a> and <a href="https://huggingface.co/datasets/Lin-Chen/MMStar">MMStar</a>, which are two popular benchmarks for VLMs that strictly require the visual information in the image to answer the question. We verify that multi-modal semantic perturbation can detect contamination with varying epochs, training strategies, and model architectures.
    <br><br>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- First row: RQA results -->
        <div class="columns is-centered" style="margin-bottom: 2rem;">
          <div class="column is-half">
            <div style="text-align: center;">
              <img id="teaser" style="width: 100%; height: auto; max-width: 100%;" src="images/rqa_llava_tight_detected.png">
            </div>
          </div>
          <div class="column is-half">
            <div style="text-align: center;">
              <img id="teaser" style="width: 100%; height: auto; max-width: 100%;" src="images/rqa_qwen_tight_detected.png">
            </div>
          </div>
        </div>
        <div class="columns is-centered" style="margin-bottom: 3rem;">
          <div class="column is-full-width">
            <p style="text-align: center; font-style: italic;"> Detection results of contaminated LLaVA-v1.5-7B and Qwen2-VL-7B on RealWorldQA. _P denotes the semantically perturbed variant. To clarify, the accuracies were measured on the 440 images after manual filtering. Detected? indicates whether the model was detected as contaminated. </p>
          </div>
        </div>

        <!-- Second row: MMStar results -->
        <div class="columns is-centered" style="margin-bottom: 2rem;">
          <div class="column is-half">
            <div style="text-align: center;">
              <img id="teaser" style="width: 100%; height: auto; max-width: 100%;" src="images/mmstar_llava_tight_detected.png">
            </div>
          </div>
          <div class="column is-half">
            <div style="text-align: center;">
              <img id="teaser" style="width: 100%; height: auto; max-width: 100%;" src="images/mmstar_qwen_tight_detected.png">
            </div>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <p style="text-align: center; font-style: italic;"> Detection results of contaminated LLaVA-v1.5-7B and Qwen2-VL-7B on MMStar. _P denotes the semantically perturbed variant. To clarify, the accuracies were measured on the 478 images after manual filtering. Detected? indicates whether the model was detected as contaminated. </p>
          </div>
        </div>
      </div>
    </div>
    
    <div class="columns is-centered" style="margin-top: 2rem;">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>From the results, we observe that:</p>
          <ol>
            <li>We do not need to manually adjust the detection threshold. Hence, our method is a practical solution to a realistic scenario where we do not know which models have been contaminated.</li>
            <li>Our method is robust to various realistic contamination strategies (e.g. models contaminated for only 1 epoch!). </li>
            <li>The performance gap between the original and perturbed variants is positively correlated with the amount of contamination, aligning with our hypothesis that contaminated models fail to generalize under controlled perturbations.</li>
          </ol>
        </div>
      </div>
    </div>
    <br>
    For a full comparison to existing detection methods, please refer to the Section 5 of our paper.
  </div>
</section>

  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Why perturbations can generate easier variants </h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->    
  <div class="container is-max-desktop">
  
    <div class="columns is-centered">
      <div class="column is-full-width">
        <centering>
          <div style="text-align: center;">
            <img id="teaser" width="95%" src="images/generalized.png">     
          </div>
        </centering>
        <div class="content has-text-justified"> 
          <p>
            Example where the perturbed variant is easier to solve than the original question. In the original image, the traffic sign is small and the text barely legible; after perturbation, the sign is enlarged and clearly visible.
            <br><br>
            When Flux ControlNet generates the perturbed image, it often highlights the salient visual cues more clearly than the original images.
            <br><br>
            We assume that by preserving the original question and only altering the answer choice, the question difficulty remains comparable. Combined with some examples like above, the perturbed dataset as a whole should have a similar or lower difficulty than the original dataset.
          </p>
        </div>  
      </div>
  </section>


  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Ablation Studies </h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->    
  <div class="container is-max-desktop">
  
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-5">Real-world Counterfactuals.</h2> 
        We use <a href="https://huggingface.co/datasets/BaiqiL/NaturalBench">NaturalBench</a>, a dataset consisting of natural adversarial counterfactual examples. We simulate our pipeline by training on one varaint of the counterfactuals and evaluating on the other variant. 
        <br><br>
        <div style="text-align: center;">
          <img id="teaser" width="95%" src="images/natbench.png">     
        </div>

        We observe that clean models show similar performance on both variants, while contaminated models show dramatic performance drops up to -45.58% (98.63% -> 53.05%). Note that this is a two-way multiple choice benchmark, meaning that a contaminated model with near perfect performance on the leaked data performs as bad as random guessing on the perturbed variant.
        <br><br>
        This result suggests that any reliable semantic variation - natural, procedural, or synthetic - fits our framework.
        <br><br>

        <h2 class="title is-5">Our pipeline is modular.</h2> For our main experiments, we use GPT-4o as the LLM and Flux ControlNet as the text-to-image model. However, we verify that our approach works even after replacing GPT-4o with Molmo-7B-D, and after replacing manual filtering with verification by a strong reasoning model like <em>o3</em>.
        <br><br>
        <h2 class="title is-5">Our pipeline generalizes to pretraining and larger models.</h2> We verify that our approach can detect contamination that occurs during the pretraining stage, when LLaVA-v1.5-7B is trained from scratch. We also verify that our approach extends to larger models like LLaVA-v1.5-13B.
        <br><br>
      </div>
    </div>
    For a full list of results on our ablation experiments, please refer to the Appendix of our paper.
  </div>
</section>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{park2025contaminationdetectionvlmsusing,
          title={Contamination Detection for VLMs using Multi-Modal Semantic Perturbation}, 
          author={Jaden Park and Mu Cai and Feng Yao and Jingbo Shang and Soochahn Lee and Yong Jae Lee},
          year={2025},
          eprint={2511.03774},
          archivePrefix={arXiv},
          primaryClass={cs.LG},
          url={https://arxiv.org/abs/2511.03774}, 
        }
  </code></pre>
    </div>
  </section>
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </section>
  <script>
    // Handle message showing
    function createChatRow(sender, text, imageSrc) {
      var article = document.createElement("article");
      article.className = "media"

      var media = document.createElement("div");
      media.className = "media-content";

      var content = document.createElement("div");
      content.className = "content";

      var para = document.createElement("p");

      // wrap text in pre tag to preserve whitespace and line breaks
      var pre_text = document.createElement("pre");
      pre_text.style = "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
      var paraText = document.createTextNode(text);
      pre_text.appendChild(paraText);

      var strong = document.createElement("strong");
      strong.innerHTML = sender;
      var br = document.createElement("br");

      para.appendChild(strong);
      para.appendChild(br);
      para.appendChild(pre_text);

      // Add image if imageSrc is provided
      if (imageSrc) {
        var vid = document.createElement("video");
        vid.width = "640"
        vid.height = "360"
        vid.setAttribute("loop", "")
        vid.setAttribute("autoplay", "")
        vid.setAttribute("controls", "")
        vid.setAttribute("muted", "")
        vid.setAttribute("defaultmuted", "")
        vid.setAttribute("playsinline", "")
        var source = document.createElement("source")
        source.src = imageSrc
        source.type = "video/mp4"
        vid.appendChild(source)
        para.appendChild(vid);
      }

      content.appendChild(para);
      media.appendChild(content);
      article.appendChild(media);
      return article;
    }

    function addMessageToChatHistory(sender, message, imageSrc) {
      const chatHistory = document.querySelector('.chat-history');
      const chatRow = createChatRow(sender, message, imageSrc);
      chatHistory.appendChild(chatRow);
      chatHistory.scrollTop = chatHistory.scrollHeight;
    }

    function clearChatHistory() {
      const chatHistory = document.querySelector('.chat-history');
      chatHistory.innerHTML = "";
    }

    // 
    const conversations = [
      {
        "description": "object",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Object"],
          ["Positive Pair", "thin air turns into fire", "videos/22_pos.mp4"],
          ["Negative Pair", "fire turns into thin air", "videos/22_neg.mp4"],
        ]
      },
      {
        "description": "action",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Action"],
          ["Positive Pair", "a toddler plays around the grass field before he picks up a water bottle and drinks", "videos/0_pos.mp4"],
          ["Negative Pair", "a toddler picks up a water bottle and drinks before he plays around the grass field", "videos/0_neg.mp4"],
        ]
      },
      {
        "description": "viewpoint",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Viewpoint"],
          ["Positive Pair", "the camera was filming the man from his right side before the video cuts to an angle 45 degrees to his right behind", "videos/326_pos.mp4"],
          ["Negative Pair", "the camera was filming the man from an angle 45 degrees to his right behind before the video cuts to his right side", "videos/326_neg.mp4"],
        ]
      },
      {
        "description": "interaction",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Interaction"],
          ["Positive Pair", "the watermelon is cut then turned", "videos/122_pos.mp4"],
          ["Negative Pair", "the watermelon is turned then cut", "videos/122_neg.mp4"],
        ]
      },
      {
        "description": "cyclical",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Cyclical"],
          ["Positive Pair", "the old chinese man writes calligraphy on a piece of paper before he dips his brush in the ink", "videos/94_pos.mp4"],
          ["Negative Pair", "the old chinese man dips his brush in the ink before he writes calligraphy on a piece of paper", "videos/94_neg.mp4"],
        ]
      },
      {
        "description": "spatial",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Spatial"],
          ["Positive Pair", "a person moonwalks from the left to the right of the camera view", "videos/473_pos.mp4"],
          ["Negative Pair", "a person moonwalks from the right to the left of the camera view", "videos/473_neg.mp4"],
        ]
      },
      {
        "description": "contextual",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["Category", "Contextual"],
          ["Positive Pair", "the man was landed before he is in the air gliding down", "videos/119_pos.mp4"],
          ["Negative Pair", "the man was in the air gliding down before he is landed", "videos/119_neg.mp4"],
        ]
      },
    ];

    // The current image index
    let currentIndex = 0;

    // The function to update the displayed chat history
    function update_dialog_demo() {
      // Clear the chat history
      clearChatHistory();

      for (let i = 0; i < conversations[currentIndex].turns.length; i++) {
        if (conversations[currentIndex].turns[i].length == 2) {
          addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1]);
        }
        else {
          addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1], conversations[currentIndex].turns[i][2]);
        }
      }

      // scroll to the top of the chat history
      document.querySelector('.chat-history').scrollTop = 0;
    }

    // Initialize the displayed image
    update_dialog_demo();

    // Event listeners for the buttons
    document.getElementById('prev-question').addEventListener('click', () => {
      currentIndex = (currentIndex - 1 + conversations.length) % conversations.length;
      update_dialog_demo();
    });

    document.getElementById('next-question').addEventListener('click', () => {
      currentIndex = (currentIndex + 1) % conversations.length;
      update_dialog_demo();
    });


  </script>


</body>

</html>
